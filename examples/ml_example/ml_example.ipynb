{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergey/anaconda3/envs/wunder_summer/lib/python3.8/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/sergey/mnt/st1500/Usr/Sergey/TheJob/Challenges/wunder_summer/wunder_challenge/scorer/orderbook_fast/orderbook_fast.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import lightgbm as lg\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../../scorer/\")\n",
    "# import orderbook as ob\n",
    "# Чтобы использовать быстрый ордербук раскомментируйте строку:\n",
    "import orderbook_fast as ob\n",
    " \n",
    "SIDE_BID = 0 \n",
    "SIDE_ASK = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем датасет для тренировки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting dataset: 100%|██████████| 10617618/10617618 [01:04<00:00, 164068.85it/s]\n",
      "collecting dataset: 100%|██████████| 10555835/10555835 [01:04<00:00, 163449.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset collected: len(X) = 234905\n",
      "Dataset collected: len(X) = 240309\n"
     ]
    }
   ],
   "source": [
    "def get_simple_features_from_orderbook(orderbook, max_index=2):\n",
    "    '''\n",
    "        Считаем простые фичи по ордербуку:\n",
    "    '''\n",
    "    spread = orderbook.get_best_price(SIDE_ASK) - orderbook.get_best_price(SIDE_BID)\n",
    "    features = [spread]\n",
    "    for side in (SIDE_BID, SIDE_ASK):\n",
    "        for ix in range(max_index):\n",
    "            price_level = orderbook.get_price_level_at_ix(side, ix)\n",
    "            if price_level is None:\n",
    "                features += [-1, -1]\n",
    "            else:\n",
    "                features += [price_level.get_volume(), \n",
    "                             price_level.get_num_orders()]\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_simple_deals_features(last_deals, orderbook):\n",
    "    '''\n",
    "        Считаем простые фичи по последним сделкам:\n",
    "    '''\n",
    "    cur_mean_price = orderbook.get_mean_price()\n",
    "    cur_time = orderbook.get_time()\n",
    "\n",
    "    features = []\n",
    "    for side in (SIDE_BID, SIDE_ASK):\n",
    "        deal_event = last_deals[side]\n",
    "        if deal_event is None:\n",
    "            features += [-1e9, -1e9, -1e9]\n",
    "        else:\n",
    "            features += [cur_mean_price - deal_event.price, \n",
    "                         cur_time - deal_event.time, \n",
    "                         deal_event.amount]\n",
    "    return features\n",
    "\n",
    "\n",
    "def collect_dataset(data_path):\n",
    "    '''\n",
    "        Собираем датасет\n",
    "    '''\n",
    "    event_player = ob.EventPlayer(data_path)\n",
    "    orderbook = ob.OrderBook()\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    last_deals = [None, None]\n",
    "    for ev in tqdm(event_player.iget_events(), \n",
    "                    total=len(event_player), \n",
    "                    desc=\"collecting dataset\"):\n",
    "        if ev.action == ob.Action.DEAL:\n",
    "            last_deals[ev.side] = ev\n",
    "        elif ev.action == ob.Action.NEW_CHUNK:\n",
    "            last_deals = [None, None]\n",
    "        \n",
    "        orderbook.apply_event(ev)\n",
    "        if ev.need_prediction:\n",
    "            features = get_simple_features_from_orderbook(orderbook)\n",
    "            features += get_simple_deals_features(last_deals, orderbook)\n",
    "\n",
    "            X.append(features)\n",
    "            Y.append(ev.Y)\n",
    "\n",
    "    print(f\"Dataset collected: len(X) = {len(X)}\")\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "X_train, Y_train = collect_dataset(\"../../data/train_small_A.npz\")\n",
    "X_test, Y_test = collect_dataset(\"../../data/train_small_B.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучаем модель градиентного бустинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.62018\tvalid_0's binary_logloss: 0.481247\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's auc: 0.62531\tvalid_0's binary_logloss: 0.479038\n",
      "[3]\tvalid_0's auc: 0.626034\tvalid_0's binary_logloss: 0.477295\n",
      "[4]\tvalid_0's auc: 0.626741\tvalid_0's binary_logloss: 0.475821\n",
      "[5]\tvalid_0's auc: 0.627089\tvalid_0's binary_logloss: 0.474582\n",
      "[6]\tvalid_0's auc: 0.627325\tvalid_0's binary_logloss: 0.473539\n",
      "[7]\tvalid_0's auc: 0.628326\tvalid_0's binary_logloss: 0.472596\n",
      "[8]\tvalid_0's auc: 0.628691\tvalid_0's binary_logloss: 0.471835\n",
      "[9]\tvalid_0's auc: 0.629272\tvalid_0's binary_logloss: 0.47112\n",
      "[10]\tvalid_0's auc: 0.629673\tvalid_0's binary_logloss: 0.470545\n",
      "[11]\tvalid_0's auc: 0.630045\tvalid_0's binary_logloss: 0.470049\n",
      "[12]\tvalid_0's auc: 0.630495\tvalid_0's binary_logloss: 0.469601\n",
      "[13]\tvalid_0's auc: 0.630722\tvalid_0's binary_logloss: 0.469228\n",
      "[14]\tvalid_0's auc: 0.631064\tvalid_0's binary_logloss: 0.468878\n",
      "[15]\tvalid_0's auc: 0.631368\tvalid_0's binary_logloss: 0.468569\n",
      "[16]\tvalid_0's auc: 0.631633\tvalid_0's binary_logloss: 0.468299\n",
      "[17]\tvalid_0's auc: 0.631749\tvalid_0's binary_logloss: 0.468105\n",
      "[18]\tvalid_0's auc: 0.631864\tvalid_0's binary_logloss: 0.467918\n",
      "[19]\tvalid_0's auc: 0.632095\tvalid_0's binary_logloss: 0.467704\n",
      "[20]\tvalid_0's auc: 0.632366\tvalid_0's binary_logloss: 0.467491\n",
      "[21]\tvalid_0's auc: 0.632673\tvalid_0's binary_logloss: 0.467298\n",
      "[22]\tvalid_0's auc: 0.632728\tvalid_0's binary_logloss: 0.467189\n",
      "[23]\tvalid_0's auc: 0.633043\tvalid_0's binary_logloss: 0.467023\n",
      "[24]\tvalid_0's auc: 0.633291\tvalid_0's binary_logloss: 0.466877\n",
      "[25]\tvalid_0's auc: 0.633419\tvalid_0's binary_logloss: 0.466746\n",
      "[26]\tvalid_0's auc: 0.633798\tvalid_0's binary_logloss: 0.466582\n",
      "[27]\tvalid_0's auc: 0.633871\tvalid_0's binary_logloss: 0.466513\n",
      "[28]\tvalid_0's auc: 0.633977\tvalid_0's binary_logloss: 0.466426\n",
      "[29]\tvalid_0's auc: 0.634046\tvalid_0's binary_logloss: 0.466348\n",
      "[30]\tvalid_0's auc: 0.634088\tvalid_0's binary_logloss: 0.466311\n",
      "[31]\tvalid_0's auc: 0.634167\tvalid_0's binary_logloss: 0.466246\n",
      "[32]\tvalid_0's auc: 0.634194\tvalid_0's binary_logloss: 0.466205\n",
      "[33]\tvalid_0's auc: 0.634259\tvalid_0's binary_logloss: 0.46617\n",
      "[34]\tvalid_0's auc: 0.634218\tvalid_0's binary_logloss: 0.46615\n",
      "[35]\tvalid_0's auc: 0.634347\tvalid_0's binary_logloss: 0.466077\n",
      "[36]\tvalid_0's auc: 0.634456\tvalid_0's binary_logloss: 0.466025\n",
      "[37]\tvalid_0's auc: 0.634585\tvalid_0's binary_logloss: 0.465955\n",
      "[38]\tvalid_0's auc: 0.634562\tvalid_0's binary_logloss: 0.465944\n",
      "[39]\tvalid_0's auc: 0.634576\tvalid_0's binary_logloss: 0.465932\n",
      "[40]\tvalid_0's auc: 0.63458\tvalid_0's binary_logloss: 0.465904\n",
      "[41]\tvalid_0's auc: 0.634626\tvalid_0's binary_logloss: 0.465878\n",
      "[42]\tvalid_0's auc: 0.63464\tvalid_0's binary_logloss: 0.465847\n",
      "[43]\tvalid_0's auc: 0.634835\tvalid_0's binary_logloss: 0.465776\n",
      "[44]\tvalid_0's auc: 0.634864\tvalid_0's binary_logloss: 0.465748\n",
      "[45]\tvalid_0's auc: 0.634865\tvalid_0's binary_logloss: 0.465746\n",
      "[46]\tvalid_0's auc: 0.634935\tvalid_0's binary_logloss: 0.465718\n",
      "[47]\tvalid_0's auc: 0.634949\tvalid_0's binary_logloss: 0.465704\n",
      "[48]\tvalid_0's auc: 0.634913\tvalid_0's binary_logloss: 0.465709\n",
      "[49]\tvalid_0's auc: 0.634855\tvalid_0's binary_logloss: 0.465715\n",
      "[50]\tvalid_0's auc: 0.634854\tvalid_0's binary_logloss: 0.4657\n",
      "[51]\tvalid_0's auc: 0.634842\tvalid_0's binary_logloss: 0.465703\n",
      "[52]\tvalid_0's auc: 0.634825\tvalid_0's binary_logloss: 0.465706\n",
      "[53]\tvalid_0's auc: 0.634831\tvalid_0's binary_logloss: 0.465697\n",
      "[54]\tvalid_0's auc: 0.634857\tvalid_0's binary_logloss: 0.465676\n",
      "[55]\tvalid_0's auc: 0.634838\tvalid_0's binary_logloss: 0.465679\n",
      "[56]\tvalid_0's auc: 0.634796\tvalid_0's binary_logloss: 0.465682\n",
      "[57]\tvalid_0's auc: 0.634796\tvalid_0's binary_logloss: 0.465677\n",
      "[58]\tvalid_0's auc: 0.634799\tvalid_0's binary_logloss: 0.465675\n",
      "[59]\tvalid_0's auc: 0.634838\tvalid_0's binary_logloss: 0.465655\n",
      "[60]\tvalid_0's auc: 0.634754\tvalid_0's binary_logloss: 0.465672\n",
      "[61]\tvalid_0's auc: 0.634716\tvalid_0's binary_logloss: 0.46568\n",
      "[62]\tvalid_0's auc: 0.634709\tvalid_0's binary_logloss: 0.46568\n",
      "[63]\tvalid_0's auc: 0.634804\tvalid_0's binary_logloss: 0.465656\n",
      "[64]\tvalid_0's auc: 0.634797\tvalid_0's binary_logloss: 0.465658\n",
      "[65]\tvalid_0's auc: 0.634738\tvalid_0's binary_logloss: 0.46567\n",
      "[66]\tvalid_0's auc: 0.634737\tvalid_0's binary_logloss: 0.465665\n",
      "[67]\tvalid_0's auc: 0.634726\tvalid_0's binary_logloss: 0.465658\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's auc: 0.634949\tvalid_0's binary_logloss: 0.465704\n"
     ]
    }
   ],
   "source": [
    "def train_classifier(X_train, Y_train, X_test, Y_test):\n",
    "    '''\n",
    "        Обучаем классификатор\n",
    "    '''\n",
    "    clf = lg.LGBMClassifier(num_leaves=31, n_estimators=1000, learning_rate=0.1)\n",
    "    clf.fit(X_train, Y_train, eval_set=[(X_test, Y_test)], \n",
    "            eval_metric=\"auc\", early_stopping_rounds=20)\n",
    "    return clf\n",
    " \n",
    "clf = train_classifier(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем получившийся классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring: 100%|██████████| 10562435/10562435 [06:37<00:00, 26586.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "roc_auc_score = 0.636\n"
     ]
    }
   ],
   "source": [
    "last_deals = [None, None]\n",
    "\n",
    "def process_event_and_predict_proba(event, orderbook):\n",
    "    if event.action == ob.Action.DEAL:\n",
    "        last_deals[event.side] = event\n",
    "    elif event.action == ob.Action.NEW_CHUNK:\n",
    "        last_deals[:] = [None, None]\n",
    "        \n",
    "    if not event.need_prediction:\n",
    "        return None\n",
    "    \n",
    "    features = get_simple_features_from_orderbook(orderbook)\n",
    "    features += get_simple_deals_features(last_deals, orderbook)    \n",
    "    proba = clf.predict_proba([features])[0, 1]\n",
    "    return proba\n",
    "\n",
    "from scorer import Scorer\n",
    "\n",
    "scoring = Scorer(\"../../data/train_small_C.npz\")\n",
    "roc_auc, (true_ys, pred_probas) = scoring.score(process_event_and_predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраним модель, и решение для отправки готово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Посмотрите код файла solution.py. \\nОн использует те же функции что и этот ноутбук, но уже готов к отправке на серверю. \\nПопробуйте создать архив с файлами solution.py и wunder.model и отправить их на проверку.'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохраним нашу модель\n",
    "clf.booster_.save_model(\"wunder.model\")\n",
    "\n",
    "'''Посмотрите код файла solution.py. \n",
    "Он использует те же функции что и этот ноутбук, но уже готов к отправке на серверю. \n",
    "Попробуйте создать архив с файлами solution.py и wunder.model и отправить их на проверку.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}